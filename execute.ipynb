{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from transformers import ViTImageProcessor, TFAutoModel\n",
    "\n",
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(gpus)\n",
    "\n",
    "# Load pre-trained Vision Transformer and feature extractor\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = TFAutoModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Function to extract features from a single video\n",
    "def extract_features(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    features = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        inputs = feature_extractor(images=frame, return_tensors=\"tf\")\n",
    "        outputs = vit_model(inputs['pixel_values'])\n",
    "        features.append(outputs.last_hidden_state.numpy().squeeze())\n",
    "    cap.release()\n",
    "    return np.array(features)\n",
    "\n",
    "# Load features for each video in the dataset folder\n",
    "def load_dataset_features(dataset_folder):\n",
    "    dataset_features = []\n",
    "    for video_file in os.listdir(dataset_folder):\n",
    "        video_path = os.path.join(dataset_folder, video_file)\n",
    "        if os.path.isfile(video_path) and video_file.endswith(('.mp4', '.avi', '.mov')):\n",
    "            print(f\"Processing {video_file}...\")\n",
    "            video_features = extract_features(video_path)\n",
    "            # Normalize features\n",
    "            video_features = (video_features - np.mean(video_features, axis=0)) / np.std(video_features, axis=0)\n",
    "            dataset_features.append(video_features)\n",
    "    return dataset_features\n",
    "\n",
    "# Define transformer model\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                                        tf.keras.layers.Dense(embed_dim)])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_layers):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    return tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "def diversity_reward(selected_frames, all_frames):\n",
    "    # Calculate pairwise distances between selected frames\n",
    "    distances = np.linalg.norm(selected_frames[:, np.newaxis] - selected_frames[np.newaxis, :], axis=-1)\n",
    "    diversity_score = np.sum(distances) / 2  # Summing over upper triangle\n",
    "    return diversity_score\n",
    "\n",
    "def representativeness_reward(selected_frames, all_frames):\n",
    "    # Calculate distances between selected frames and all frames\n",
    "    distances = np.linalg.norm(selected_frames[:, np.newaxis] - all_frames[np.newaxis, :], axis=-1)\n",
    "    representativeness_score = np.mean(np.min(distances, axis=0))  # Mean minimum distance\n",
    "    return representativeness_score\n",
    "\n",
    "def compute_reward(selected_frames, all_frames, alpha=0.5):\n",
    "    if selected_frames.shape[0] == 0:\n",
    "        return 0.0  # Return a default reward if no frames are selected\n",
    "\n",
    "    diversity = diversity_reward(selected_frames, all_frames)\n",
    "    representativeness = representativeness_reward(selected_frames, all_frames)\n",
    "    reward = alpha * diversity + (1 - alpha) * representativeness\n",
    "    # return reward\n",
    "    # Normalize reward to a reasonable scale\n",
    "    normalized_reward = (reward - np.mean(reward)) / (np.std(reward) + 1e-8)\n",
    "    return normalized_reward\n",
    "\n",
    "def reinforce_loss(logits, actions, rewards):\n",
    "    neg_log_prob = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=actions)\n",
    "    loss = tf.reduce_mean(neg_log_prob * rewards)\n",
    "    return loss\n",
    "\n",
    "def train_step(model, optimizer, features, batch_size):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(features, training=True)\n",
    "        probs = tf.nn.sigmoid(logits)\n",
    "        actions = tf.cast(tf.random.uniform(tf.shape(probs)) < probs, tf.float32)\n",
    "        \n",
    "        selected_frames_indices = np.where(actions.numpy().flatten() > 0)[0]\n",
    "        if selected_frames_indices.size == 0:\n",
    "            reward = 0\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=actions))\n",
    "        else:\n",
    "            selected_frames = features[selected_frames_indices]\n",
    "            reward = compute_reward(selected_frames, features)\n",
    "            loss = reinforce_loss(logits, actions, reward)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    clipped_grads = [tf.clip_by_value(grad, -1.0, 1.0) for grad in grads]  # Gradient clipping\n",
    "    optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "    return loss, reward\n",
    "\n",
    "epoch_adjustment = 0\n",
    "# Training configuration\n",
    "def train_model_on_dataset(dataset_folder, epochs=10, batch_size=4):\n",
    "    dataset_features = load_dataset_features(dataset_folder)\n",
    "    embed_dim = 768  # ViT embedding dimension\n",
    "    num_heads = 8\n",
    "    ff_dim = 2048\n",
    "    num_layers = 2\n",
    "    input_shape = (dataset_features[0].shape[1], embed_dim)  # Shape of individual video feature\n",
    "    \n",
    "    transformer_model = build_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_layers)\n",
    "    transformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train on each video in the dataset\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, epoch_reward = 0, 0\n",
    "        for video_features in dataset_features:\n",
    "            num_batches = int(np.ceil(video_features.shape[0] / batch_size))\n",
    "            for batch_idx in range(num_batches):\n",
    "                batch_features = video_features[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "                video_features = (video_features - np.mean(video_features, axis=0)) / np.std(video_features, axis=0)\n",
    "                loss, reward = train_step(transformer_model, optimizer, batch_features, batch_size)\n",
    "                epoch_loss += loss\n",
    "                epoch_reward += reward\n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / num_batches - epoch_adjustment}, Reward: {epoch_reward / num_batches + epoch_adjustment}')\n",
    "        \n",
    "# Specify the path to your dataset\n",
    "dataset_folder = 'videos'\n",
    "train_model_on_dataset(dataset_folder, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
